---
layout: default
title: DREAMT | Evaluation
---
<div class="page-header">
  <h1>Evaluation<font color="lightgrey"> : Challenge Problem 3</font></h1>
</div>

<p>Machine translation systems are typically evaluated through relative
ranking. For instance, given the following German sentences:
</p>

<center>
<p><i>Die Prager Börse stürzt gegen Geschäftsschluss ins Minus. <br/>
Nach dem steilen Abfall am Morgen konnte die Prager Börse die Verluste korrigieren. 
</i></p>
</center>

<p>...One machine translation system produces this output:</p>
<center>
<p><i>The Prague stock exchange throws into the minus against closing time. <br/>
After the steep drop in the morning the Prague stock exchange could correct the losses. 
</i></p>
</center>

<p>...And a second machine translation system produces this output:</p>
<center>
<p><i>The Prague stock exchange risks geschäftsschluss against the down. <br/>
After the steilen waste on the stock market Prague, was aez almost half of the normal tagesgeschäfts. 
</i></p>
</center>

<p>A plausible ranking of the translation systems would place the first
system higher than the second. While neither translation is perfect, the 
first one is clearly easier to understand and conveys more of the original
meaning. <b>Your challenge is to write a program that ranks the systems in 
the same order that a human evaluator would</b>. 
</p>

<p>There is great need to evaluate translation systems: to 
decide whether to purchase one system or another, or to assess incremental
changes to a system &mdash; since, as you saw in the first two assignments,
there are many choices in the design of a system that naturally lead to
different translations. Ideally such comparisons between systems should be 
done by humans, but human rankings are slow and costly to obtain, making 
them less feasible when comparisons are must be done frequently or between 
large numbers of systems. Furthermore, as you saw in class, it is possible
to use machine learning techniques to directly optimize machine translation 
towards an objective function. If we could devise a function that 
correctly ranked systems, we could, in principle achieve better translation
through optimization. Hence automatic evaluation is a topic of intense study.
</p>


<a name="starting"></a>
<h2>Getting Started</h2>

<p>If you already have a clone of the repository from previous challenges
you are ready to start. If you don't, clone a fresh version of the repository by running:</p>

<p><pre>git clone https://github.com/alopez/dreamt.git</pre></p>

<p>Under the new <tt>evaluate</tt> directory, We have provided you with a very 
simple evaluation program written in Python. There is also a directory containing
a development dataset and a test dataset. Each dataset consists of 
a human translation and many machine translations of some German documents.
The evaluator compares each machine translation to a human 
<i>reference translation</i> sentence by sentence, computing how many words 
they have in common. It then ranks the machine translation systems according 
to the percentage of words that also appear in the reference. Note that while we
collect statistics for each sentence, and the best system on each sentence
will vary, the final ranking is at the system level.
Run the evaluator on the development data using this command:</p>

<p><pre>evaluate &gt; output</pre></p>

<p>This runs the evaluation and stores the final ranking in 
<tt>output</tt>. You can see the rank order of the systems simply by looking
at the file &mdash; the first line is the best system, the second is second
best, and so on. To calculate the correlation between this ranking and a 
human ranking of the same set of systems, run the command:
</p>

<p><pre>grade &lt; output</pre></p>

<p>This command computes 
<a href="http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient">
Spearman's rank correlation coefficient</a> (\(\rho\)) between the automatic ranking
and a human ranking of the systems (see Section 4 of 
<a href="http://aclweb.org/anthology-new/W/W11/W11-2103.pdf">this paper</a> for an explanation
of how the human rankings were obtained). A \( \rho \) of 1 means that the rankings
are identical; a rank of zero means that they are uncorrelated; and a negative
rank means that they are inversely correlated.</p>

<p>You should also rank the test data, for which we did not provide human
rankings. To do this, run the command:</p>

<p><pre>evaluate -d data/test &gt; output</pre></p>

<p>You can confirm that the output is a valid ranking of the test data 
using the check command:</p>

<p><pre>check &lt; output</pre></p>

<p>Your ranking should be a total ordering of the systems &mdash;
ties are not allowed.
</p>

<a name="challenge"></a>
<h2>The Challenge</h2>

<p>Improving the evaluation algorithm should cause \(\rho\)
to increase. Your task for this assignment is to <b>obtain a
Spearman's rank correlation coefficient that is as high as possible on the 
test data.</b> Whoever obtains the highest \(\rho\) will receive the most 
points. 
<p>

<p>One way to improve over the default system is to implement the 
well-known <a href="http://aclweb.org/anthology-new/P/P02/P02-1040.pdf">
BLEU</a> metric. You may find it useful to experiment with BLEU's parameters,
or to retokenize the data in some way. However, there are many, many 
alternatives to BLEU &mdash; the topic of evaluation is so popular that
Yorick Wilks, a well-known researcher, once remarked that  
<i>more has been written about machine translation evaluation than about
machine translation itself</i>. Some of the techniques people have tried
may result in stronger correlation with human judgement, including:
</p> 

<ul class="real">
  <li><a href="http://aclweb.org/anthology-new/W/W11/W11-2105.pdf">Incorporating recall statistics into the metric.</a></li>
  <li><a href="http://aclweb.org/anthology-new/W/W07/W07-0734.pdf">Stemming the words, or counting synonyms as matches.</a></li>
  <li><a href="http://aclweb.org/anthology-new/W/W11/W11-2112.pdf">Analyzing predicate-argument structure and distributional semantics of the translations</a>.</li>
  <li><a href="http://aclweb.org/anthology-new/W/W11/W11-2106.pdf">Combining many of these features with machine learning techniques</a> (you could train on the development data).</li>
  <li><a href="http://aclweb.org/anthology-new/W/W11/W11-2113.pdf">Combining only simple features with machine learning</a>.</li>
</ul>

<p>But the sky's the limit! Evaluation isn't a solved problem, and you 
   should experiment to see how well you can do. Who knows? Maybe you'll
   develop the next state-of-the-art evaluation algorithm!
</p>


